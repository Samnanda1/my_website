---
categories:
- ""
- ""
date: "2017-10-31T22:26:13-05:00"
description: "Click here to see my project on tennis data"
draft: false
image: coding.jpg
keywords: ""
slug: blog1
title: Coding
---


```{r}
load("tennis.RData")
tennis
View(tennis)
set.seed(8009488)
sampid = sample(dim(tennis)[1], 250)
tennisdata=tennis[sampid, ]
dim(tennisdata)

head(tennisdata) ## winpct column 19
(n=nrow(tennisdata))
(p=ncol(tennisdata)-1) ## 18 explanatory variables
```

## **Project - Tennis Data**

***Summary***

***Introduction***
The following report is an analysis of 250 professional male tennis players from the 2017 season. The report focuses on building and interpreting a linear regression model for predicting the win percentage (WinPct), based on the given data. For each tennis player there are 18 variables taken into consideration; for example, number of aces, number of double faults and service points won. A full list of variables can be found in the appendix. 

***Method***
A number of methods, such as exploratory plots, subset selection, LASSO, partial least squares and 10-fold cross validation. These have all been modelled using R.
The response variable is WinPct and is stored in column 19, and there are 18 explanatory variables. The explanatory variables are not transformed, i.e., they are in their raw form. (The means aren’t equal to zero and the standard deviations aren’t equal to one).

***Exploratory Plots***
Before proceeding with our regression analysis, it is important to do prior exploratory analysis to understand the relationship between the response and explanatory variables. Using the scatterplot matrix below, it allows us to depict the pairwise dependencies of the variables. 
Each individual box depicts a scatter plot of 2 of the variables, for example the red highlighted box shows service games won against service points won. 

```{r}
pairs(tennisdata) ## scatter plot analysis 4 1b and pg 11 of notes

cor(tennisdata) ## sample corr matrix, verify what we thought?, 1c
```

Using the usual interpretation of scatter plots, it demonstrates that there is a strong positive linear relationship between service games won and service points won, i.e., as one increases so does the other. Many of the variables are not correlated as seen by the lack of patterns on the graphs, e.g., the percentage of first serves in play does not affect the number of points won on the 2nd serve.

To confirm the above, the correlation can be checked between variables, the correlation between service games won and service points won is 0.96, which shows a near perfect correlation, supporting the above findings. 

***Fitting the Full MLR Model***
Using R, the full multiple linear regression model has been calculated. As the full model contains 18 variables, the coefficients are very small. This is to be expected as changing 1 variable whilst keeping the others constant wouldn’t have too much effect on the overall outcome. For example, B1 = 1.675e-05, which can be interpreted as for every extra ace, the win percentage increases by 1.1%, as long as the other factors remain constant. From the coefficients, it can be seen that half of the variables decrease the overall win percentage. For example, the more break points faced, the win percentage decreases by 0.03%. 

Using the p-value we can test, the significance of each variable.

The table of p-values tests:

		H0: Bi = 0     		vs 		H1: Bi ≠ 0 

If we consider the explanatory variables one at a time, there are several which would not need to be in a model containing all the other variables. The majority of the variables have large p values, suggesting that when considered one at a time, each of these variables contributes a little amount to the model. In our model, Return Games Won is the most significant factor in the model, followed by Frist service return points won and break points saved. This means that they influence the model the greatest. This tells us we do not need to include all variables in the model. 

R2, the coefficient of determination, is used to determine how much variability of one factor is caused by other factors. It can sometimes be referred to as a ‘goodness of fit’, with 1 being a perfect fit. Our model has an R2 of 0.4914 indicating that the full model may not be a good fit. 

Using the predict command, y-hat values have been generated. The mean percentage win, of the predicted values from R is 42.19%. This is a close match to the mean win percentage of the entire data; 42.44%. Despite R2 not being perfect, as these percentages are quite close the model might be better at predicting future values then at first glance.

A full list of the coefficients and p-values can be found in the appendix.

Model Selection
As previously determined, using p-values, some of the variables are not necessarily important in determining the overall win percentage. Therefore, using best subset selection, a model with less variables can be created. This should ensure the model is easier to interpret and increase the model accuracy. 

***Best Subset Selection***
Best subset selection involves using least squares to fit a linear regression model to each possible subset of the 18 explanatory variables. I.e., the null model, full model, all 18 models with a single predictor are fitted and all p(p-1)/2 model with two predictors are fitted and so on. The 218 possible models are then examined to determine the best model. Using the regsubset function we can determine the best model given Adjusted R2 , Mallows Cp and Bayes Information Criterion. This is graphically demonstrated below.

```{r}
lsq_fit = lm(WinPct ~ ., data=tennisdata) ## fitting model via least squares, analysis 1d
(lsq_summary = summary(lsq_fit))

yhat=predict(lsq_fit,tennisdata)
head(yhat)
head(tennisdata$WinPct)
```

For Adjusted R2, the best model occurs when the line plot is maximised, this occurs when the model has 12 predictors, however it can be noted that models 10 through 14 produce similar results. For Mallows Cp and BIC, best model selection occurs when the line graph is minimised which is at 10 and 4 respectively. It often occurs that the 3 practices choose different models. However, as the Adjusted R2 models 10 and 12 are very similar we could therefore say the best model match in fact may be the one with 12 variables. Using a model with 12 variables, indicates the most significant are First Serve Percentage, Break Points Saved, Service Games Won, First Service Return Points Won, Break Point Opportunities, Total Points Won, Aces, First Serve won Percentage, Service Games played, Service Points Won, Second Serves Won and Return games won. 

The coefficients for these 12 variables can be found in the Appendix. 

Aces, First Serve Percentages, First Serves Won, Service Games Won, Break Point Opportunities, Return Games Won and Total Points Won, all have positive coefficients, indicating that as these increase the Win Percentage increases. For the full model, gaining 1 extra ace increased your win percentage by 1.1%, however for a 12-variable model, greater significance is placed on gaining an ace, and therefore it increases your win percentage by 1.9%.

BIC doesn’t agree with the above, and therefore more test have been done to confirm whether a 12 variable model is favorable to one with fewer variable.

***Cross Validation of Adjusted R2, Cp and BIC***

To test the predictive performance of the above models, i.e., how close the predicted values and the actual values are, cross validation has been used. The Test Error: The average error that results from predicting the response for an observation that was not used in model fitting is calculated and used as it measures the predicative performance of the model on previously unseen data. 

To do this, y-hat is fitted for each of the 18 models, we then compare the average mean squared error (MSE) for each of these models and the one that minimises the test error the most is judged to be the ‘best’. Using R, the MSE has been calculated for each model (See Appendix). The model which decreases the MSE the most is model 3, with an error of 0.01109. This is depicted in the graph below. 

This analysis contradicts the above findings, and supports the findings using the BIC method more as the MSE for a 4 variable model is similar to that of one with 3.

```{r}
set.seed(1)
nfolds=10
foldindex = sample(nfolds, n, replace = TRUE)
head(foldindex)

foldsize = numeric(nfolds)
which(foldindex==9) ## seeing which no.s are in group 9
for (k in 1:nfolds)  foldsize[k] = length(which(foldindex==k)) 
foldsize

cvlsqerrors= numeric(nfolds)

for (k in 1:nfolds) {
  lsqtmpfit = lm(WinPct ~ ., data=tennisdata[foldindex!=k,])
  lsqtmppredict = predict(lsqtmpfit, tennisdata[foldindex==k,])
  cvlsqerrors[k] = mean((tennisdata[foldindex==k,]$WinPct - lsqtmppredict)^2)
}
cvlsqerrors ##code doesn't work

(lsqfinalmse = weighted.mean(cvlsqerrors, w=foldsize))

##10 fold cross validation to estimate the average MSE under predictive performance, 1e
```

The 3 variable model can be found in the appendix and can be interpreted as; if service games won, break point opportunities and return games won, each increase by 1 then the win percentage increases by 4.46%

***Lasso***

Given the contradiction in findings between the best subset of variables, Lasso is another model selection tool, which has been used. Lasso has been used instead of ridge regression, as it uses a subset of the variables instead of the full model. It does this by setting some of the coefficients to zero. 

λ is introduced, into the equation, and it controls the strength of the penalty term. Small values apply small penalties and give solutions close to the least square, and large values apply large penalties and give solutions which are strongly shrunk toward zero. A range of λ = 10^5 (lots of shrinkage) and λ = 10^-3 (little shrinkage) has been used.

When λ = 105, the regression coefficient for each explanatory variable has essentially been shrunk to zero. This is the null model.

When λ = 0.001 none of the coefficients equal zero and we get a solution very close to the least squares estimate.

When λ= 0.1047616 the coefficients are either zero or been shrunk toward zero to the least square’s solution.

```{r}
install.packages("leaps")

library(leaps)

bssfit=regsubsets(WinPct ~ ., data=tennisdata, method="exhaustive", nvmax=p)
(bsssummary = summary(bssfit))

bsssummary$adjr2

bsssummary$cp

bsssummary$bic

(bestadjr2 = which.max(bsssummary$adjr2))

(bestcp = which.min(bsssummary$cp))

(bestbic = which.min(bsssummary$bic))
## best subset selection, and one with smallest sum of sqaures, why select it, which variabels woudl be used q2a
```

The plot above shows how the estimated coefficients vary as a function of λ. As the tuning parameter λ increases we see that variables 1,2,6,8,13 and 15 drop out almost instantly. As λ continues to increase variable 17 and then 10 drop out and so. The last variables to drop out are 16 (Return Games Won) and 14(Breakpoints Converted). This indicates the ones that drop out first are the less influential in determining the win percentage compared to those that drop out last. 

To obtain a suitable tuning parameter, cross validation is used, which produces the plot below.

```{r}
## multi panel plotting device for the above
par(mfrow=c(2,2))
plot(1:p, bsssummary$adjr2, xlab = "Number of Predictors", ylab = "Adjusted Rsq", type="b")
points(bestadjr2,bsssummary$adjr2[bestadjr2], col="red", pch=16)

plot(1:p, bsssummary$cp, xlab = "Number of Predictors", ylab = "Cp", type="b")
points(bestcp,bsssummary$cp[bestcp], col="red", pch=16)

plot(1:p, bsssummary$bic, xlab = "Number of Predictors", ylab = "BIC", type="b")
points(bestbic,bsssummary$bic[bestbic], col="red", pch=16)

par(mfrow=c(1,1))

coef(bssfit,18)
```


```{r}
lsqtmppredict = predict(lsqtmpfit,tennisdata[foldindex==k,])
class(lsq_fit)
class(bssfit)

predict.regsubsets  =function(object, newdata, id, ...){
  form = as.formula(object$call[[2]])
  mat=model.matrix(form, newdata)
  coefi = coef(object, id=id)
  xvars = names(coefi)
  return(mat[,xvars]%*% coefi)
}

cvbsserrors = matrix(NA, p, nfolds)

for (k in 1:nfolds) {
  bsstmpfits = regsubsets(WinPct ~ ., data=tennisdata[foldindex!=k,], method = "exhaustive",nvmax=p)
  for (m in 1:p) {
  bsstmppredict = predict(bsstmpfits,tennisdata[foldindex==k,], m)
  cvbsserrors[m,k] = mean((tennisdata[foldindex==k,]$WinPct - bsstmppredict)^2)
  }
}

cvbsserrors ## code doesn't work again 

bss_mse=numeric(p)

for(m in 1:p){
  bss_mse[m]=weighted.mean(cv_bss_errors[m,],w=foldsize)
  }
bss_mse ##code doesn't work

(best_cv=which.min(bss_mse))

par(mfrow=c(2,2))
plot(1:p, bsssummary$adjr2,xlab="Number of predictors",ylab="Adjusted Rsq",type="b")
points(bestadjr2, bsssummary$adjr2[bestadjr2],col="red",pch=16)
plot(1:p, bsssummary$cp,xlab="Number of predictors",ylab="Cp",type="b")
points(bestcp, bsssummary$cp[bestcp],col="red",pch=16)
plot(1:p, bsssummary$bic,xlab="Number of predictors",ylab="BIC",type="b")
points(bestbic, bsssummary$bic[bestbic],col="red",pch=16)
plot(1:p, bss_mse,xlab="Number of predictors",ylab="10-fold CV Error",type="b")
points(best_cv, bss_mse[best_cv],col="red",pch=16)
par(mfrow=c(1,1))

bss_18_predict=predict(bssfit, tennisdata,18)

mean((tennisdata$WinPct-bss_18_predict)^2)
```

Using this tuning parameter a ‘best’ model can be created. A 3 variable model is the ‘best’ model for the data. All variables apart from Ace’s, Break Points Faced and Break Point Opportunities have been forced out the model. For coefficients see Appendix. 

Unlike the previous 3 variable model, this model contains break points faced, however it has a negative coefficient. This indicates that the more break points faced, the win percentage decreases. The other 2 variables increase the win percentage as they increase.

The test error under cross-validation is less than that under least squares, but not as small as that found using best subsets selection.

***Partial Least Squares (PLS)***

Finally, PLS is used, it identifies new principal components that summarise the original predictors and also ones that are related to the outcome. These components are then used to fit the regression model. 

The summary function allows us to see how much of the variation is explained by each of the transformed variables. Using one transformed explanatory variable explains 35% of the variation in y, which is fairly low, at 3 variables it is at 37.62%, at 12 variables it is 48.74%. It is not until 15 variables are in the model that the variation in y is fully explained. 

We choose the number of transformed explanatory variables using cross validation as seen in the graph below.

In this case there is a clear elbow at 3 components so we could select p= 3 giving a model which uses the first 3 PLS directions for our transformed explanatory variables. The coefficients of this can be seen in the appendix. 

***Cross Validation of all Models***
The best method and therefore model can be identified using cross validation between the models. This is the model which gives the lowest MSE. It is already known a 3 variable model using best subset selection is of 0.01109. LASSO gives an MSE of 0.01077 and PLS is 0.012. Therefore the LASSO 3 variable model should be used.

***Conclusion and Evaluation***

***Model Choice***
Initially given best subset selection we would have chosen a model with 12 variables however with further investigation the model selected would be the LASSO 3 variable model; Aces, Double Faults and First Serve Percentage. This model has been chosen because, the BIC model selection suggested 4 variables, but the difference in models between 3 and 4 is very low as depicted by the graph. Equally the MSE is reduced the greatest with model 3, and the 3 variable MSE for models using PLS and Lasso are low. Having only 3 variables makes the model easier to interpret.

***Criticism***
The main problem with best subset selection is that the number of models to be considered grows geometrically with the number of explanatory variables p. For example, with 18 explanatory variables leads to 218 possible models. Therefore 262,144 models would need to be considered, which is computationally hard. For 40 variables, this method becomes impossible. 

***Improvement***
Given there were 50 more entries in the entire data set, it would be useful to re do the analysis but with multiple different subsets of data, to see if the same results are obtained. This would further the validity of the model choice. To improve the results further, more model selection tests could be done, for example Ridge Regression and PCR. 


